{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScanBot\n",
    "\n",
    "The AI-driven bot that will read all your scanned documents and extract information into a database automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Follow this notebook and add missing code sections denoted by \"TODO\".\n",
    "To get a high-level understanding of the ScanBot, jump to the **Main** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai (from -r requirements.txt (line 1))\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/48/62/63419a90b502aa5a590c83ffbd30f0a93fe15aeb63d0374e898e612f4f03/openai-1.30.5-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.30.5-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from -r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from -r requirements.txt (line 3)) (2.2.1)\n",
      "Collecting openpyxl (from -r requirements.txt (line 4))\n",
      "  Obtaining dependency information for openpyxl from https://files.pythonhosted.org/packages/58/d9/796181a30827b12101786c21301f0f4536597a9249530916b1fdb5bbad91/openpyxl-3.1.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading openpyxl-3.1.3-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting python-dotenv (from -r requirements.txt (line 5))\n",
      "  Obtaining dependency information for python-dotenv from https://files.pythonhosted.org/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting regex (from -r requirements.txt (line 6))\n",
      "  Obtaining dependency information for regex from https://files.pythonhosted.org/packages/8b/ee/05f14a99a81f1a897a9146f3f565efb116ad6412f875f52e895c02666825/regex-2024.5.15-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading regex-2024.5.15-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.3.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (0.27.0)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for pydantic<3,>=1.9.0 from https://files.pythonhosted.org/packages/6f/b9/ec44b1394957d5aa8d3a7c33f8304cd7670d10a43a286db56cec086346be/pydantic-2.7.2-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.7.2-py3-none-any.whl.metadata (108 kB)\n",
      "     ---------------------------------------- 0.0/108.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 108.5/108.5 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2024.1)\n",
      "Collecting et-xmlfile (from openpyxl->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for et-xmlfile from https://files.pythonhosted.org/packages/96/c2/3dd434b0108730014f1b96fd286040dc3bcb70066346f7e01ec2ac95865f/et_xmlfile-1.1.0-py3-none-any.whl.metadata\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (0.14.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.3 (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for pydantic-core==2.18.3 from https://files.pythonhosted.org/packages/e3/5c/477dac00c0d6d34921fec2507ae6aea2cd7c84072eab1dca5bcbbf86c4a2/pydantic_core-2.18.3-cp312-none-win_amd64.whl.metadata\n",
      "  Downloading pydantic_core-2.18.3-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\d92343\\appdata\\local\\mambaforge\\envs\\azure\\lib\\site-packages (from tqdm>4->openai->-r requirements.txt (line 1)) (0.4.6)\n",
      "Downloading openai-1.30.5-py3-none-any.whl (320 kB)\n",
      "   ---------------------------------------- 0.0/320.7 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 153.6/320.7 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 320.7/320.7 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading openpyxl-3.1.3-py2.py3-none-any.whl (251 kB)\n",
      "   ---------------------------------------- 0.0/251.3 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 194.6/251.3 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  245.8/251.3 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  245.8/251.3 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  245.8/251.3 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 251.3/251.3 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading regex-2024.5.15-cp312-cp312-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.5 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 245.8/268.5 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 268.5/268.5 kB 5.5 MB/s eta 0:00:00\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading pydantic-2.7.2-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 0.0/409.5 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 256.0/409.5 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 409.5/409.5 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.3-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.9 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 8.0 MB/s eta 0:00:00\n",
      "Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: regex, python-dotenv, pydantic-core, et-xmlfile, distro, annotated-types, pydantic, openpyxl, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 et-xmlfile-1.1.0 openai-1.30.5 openpyxl-3.1.3 pydantic-2.7.2 pydantic-core-2.18.3 python-dotenv-1.0.1 regex-2024.5.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import base64\n",
    "import re\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert your API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"sk-proj-iHrdMRQrydnSDtQUIzU6T3BlbkFJCoAngxToaT2????????\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load prompts from files\n",
    "def load_prompt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "# Function to analyze the document and extract meta data\n",
    "def analyze_document(file_path, prompt):\n",
    "    # create image data url\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        image_url = 'TODO...' # TODO! Hint: How can you send an image via an API?\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                # TODO! Hint: Use ChatGPT playground or Docs\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_tokens=256,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # hard coded sleep time to avoid rate limit issues\n",
    "        time.sleep(15)\n",
    "        return analyze_document(file_path, prompt)\n",
    "\n",
    "\n",
    "# Function to generate a filename based on meta data\n",
    "def generate_filename(meta_data):\n",
    "    # <doc type>_<date>_<title>.jpg\n",
    "    # if any of the fields are missing, use \"unknown\"\n",
    "    string = \"\"\n",
    "    string += str(meta_data.get(\"document_type\", \"unknown\")) + \"_\"\n",
    "    string += str(meta_data.get(\"author\", \"unknown\")) + \"_\"\n",
    "    string += str(meta_data.get(\"date\", \"unknown\")) + \"_\"\n",
    "    string += str(meta_data.get(\"title\", \"unknown\"))\n",
    "    # Make sure that the filename is a valid filename\n",
    "    string = re.sub(r\"[^\\w\\s-]\", \"\", string)\n",
    "    return string + \".jpg\"\n",
    "\n",
    "\n",
    "# Function to save meta data to a YAML file\n",
    "def save_meta_file(meta_data, meta_file_path):\n",
    "    # Save json file as yaml\n",
    "    with open(meta_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        yaml.dump(meta_data, file, default_flow_style=False,\n",
    "                  allow_unicode=True)\n",
    "\n",
    "\n",
    "# Function to save meta data to CSV and Excel files\n",
    "def save_meta_data_to_csv_excel(meta_data_list, csv_path, excel_path):\n",
    "    # check if the csv file exists\n",
    "    if not os.path.exists(csv_path):\n",
    "        existing_df = pd.DataFrame()\n",
    "    else:\n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "\n",
    "    new_df = pd.DataFrame(meta_data_list)\n",
    "    merged_df = pd.concat([existing_df, new_df]).drop_duplicates(subset='original_filepath')\n",
    "    merged_df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "    merged_df.to_excel(excel_path, index=False)\n",
    "\n",
    "\n",
    "def parse_meta_data(response):\n",
    "    # Parse the YAML string from the response\n",
    "    yaml_string = \"TODO\" # TODO get the yaml content between \"```yaml\" and \"```\"\n",
    "    meta_info = \"TODO\" # TODO use regular expression to remove \\n characters\n",
    "    # When the YAML string is in the correct format, it can be loaded as a JSON object\n",
    "    meta_json = yaml.safe_load(meta_info)\n",
    "    return meta_json\n",
    "\n",
    "\n",
    "def save_meta_data(meta_json, file_path):\n",
    "    # Generate new file name\n",
    "    new_filename = generate_filename(meta_json)\n",
    "    # Rename the document file\n",
    "    renamed_file_location = os.path.join('database', 'renamed files', new_filename)\n",
    "    shutil.copy2(file_path, renamed_file_location)\n",
    "    # Create meta data file\n",
    "    meta_file_path = os.path.join('database', 'meta files', new_filename.split('.')[0] + \".yaml\")\n",
    "    save_meta_file(meta_json, meta_file_path)\n",
    "    # Add new file name to meta data\n",
    "    meta_json[\"original_filepath\"] = file_path\n",
    "    meta_json[\"renamed_filename\"] = new_filename\n",
    "    return new_filename\n",
    "\n",
    "def get_original_filenames(csv_path):\n",
    "    # Extract all \"original_filepath\" from the meta_data csv file\n",
    "    original_filepaths = []\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        original_filepaths = df[\"original_filepath\"].tolist()\n",
    "    return original_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script for Document Analysis and Meta Data Generation\n",
    "\n",
    "This script performs the following steps:\n",
    "1. Load the scanned document from the specified directory.\n",
    "2. Analyze the document using the OpenAI GPT-4o model.\n",
    "3. Generate meta data for the document including type, date, author, recipient,\n",
    "   title, keywords, summary, entities, language, and page count.\n",
    "4. Save the meta data as a YAML file.\n",
    "5. Generate an informative file name based on the meta data.\n",
    "6. Rename the document file with the newly generated informative name.\n",
    "7. Finally, create a csv and excel file with all the meta data.\n",
    "\n",
    "Project Structure:\n",
    "- database/\n",
    "  - meta files/\n",
    "  - renamed files/\n",
    "- documents/\n",
    "  - <scanned_documents>.jpg\n",
    "- prompts/\n",
    "  - analyze_document.txt\n",
    "  - generate_title.txt\n",
    "- .env\n",
    "- .gitignore\n",
    "- main.py\n",
    "- README.md\n",
    "\n",
    "Instructions:\n",
    "1. Place your scanned/uploaded documents in the 'documents' directory.\n",
    "2. Modify the prompt in 'prompts/analyze_document.txt' according to your requirements.\n",
    "3. Run all cells in the jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the next cell will delete previous extractions from the `database` directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete everything inside the database folder in one line\n",
    "shutil.rmtree('database', ignore_errors=True)\n",
    "\n",
    "# First, make sure that database/meta files and database/renamed_files exist\n",
    "os.makedirs('database/meta files')\n",
    "os.makedirs('database/renamed files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checkout the prompt first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please analyze the attached scan of a document and extract the relevant information. Create a metadata file in YAML format that includes only the following fields and nothing else:\n",
      "\n",
      "- document_type: The type or category of the document (e.g., invoice, receipt, contract, letter).\n",
      "- date: The date of the document, if available (e.g., invoice date, letter date).\n",
      "- author: The person or organization that created or sent the document.\n",
      "- recipient: The person or organization to whom the document is addressed.\n",
      "- title: The main title or subject of the document.\n",
      "- keywords: Important terms or phrases extracted from the document to facilitate searching.\n",
      "- summary: A brief summary of the content of the document.\n",
      "- entities: Important entities mentioned in the document (e.g., names, places, organizations).\n",
      "- page: The page number of the document (e.g., page 1 of 3).\n",
      "\n",
      "The response must be strictly in YAML format and contain only the metadata fields specified above, without any additional text or explanations in your response. Here is an example of the expected output in YAML format:\n",
      "\n",
      "```yaml\n",
      "document_type: Invoice\n",
      "date: 2020-12-01\n",
      "author: John Doe\n",
      "recipient: Jane Smith\n",
      "title: December Invoice for Services\n",
      "keywords: [invoice, December, payment]\n",
      "summary: This is the invoice for December 2020 services. It mentions repair services for bikes.\n",
      "entities: [John Doe, Jane Smith, XYZ Corporation]\n",
      "page: 1\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt_path = \"prompts/analyze_document.txt\"\n",
    "prompt = load_prompt(prompt_path)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ChatGPT-4o to collect information about all scanned documents; subsequently collect all data in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta data saved to database/meta_data.csv and database/meta_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "meta_data_list = []\n",
    "document_dir = \"documents\"\n",
    "csv_path = \"database/meta_data.csv\"\n",
    "excel_path = \"database/meta_data.xlsx\"\n",
    "# Extract all \"original_filepath\" from the meta_data csv file\n",
    "original_filepaths = get_original_filenames(csv_path)\n",
    "\n",
    "for filename in os.listdir(document_dir):\n",
    "    # We want to extract the information from each scanned document in the documents folder\n",
    "    extensions = (\".jpg\", \".jpeg\", \".png\")\n",
    "    if (filename.endswith(extensions) and\n",
    "        (os.path.join(document_dir, filename) not in original_filepaths)):\n",
    "        file_path = os.path.join(document_dir, filename)\n",
    "        print(f\"Processing {filename}...\")\n",
    "        # Analyze the document to get meta data\n",
    "        llm_response = analyze_document(file_path, prompt) # TODO: Implement this function\n",
    "        # Parse the meta data from the response\n",
    "        meta_json = parse_meta_data(llm_response) # TODO: Implement this function\n",
    "        # Save the meta data and rename the document file\n",
    "        new_file_name = save_meta_data(meta_json, file_path)\n",
    "        meta_data_list.append(meta_json)\n",
    "        print(f\"Processed {filename}, renamed it to {new_file_name}\")\n",
    "\n",
    "# Save all meta data to CSV and Excel files\n",
    "save_meta_data_to_csv_excel(meta_data_list, csv_path, excel_path)\n",
    "print(f\"Meta data saved to {csv_path} and {excel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put your own scans into the `documents` folder. Did ChatGPT extracted the contents correctly? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
