{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to google colab to make it environment independent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\python310\\lib\\site-packages (from -r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: pyyaml in c:\\python310\\lib\\site-packages (from -r requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\d92343\\appdata\\roaming\\python\\python310\\site-packages (from -r requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: openpyxl in c:\\python310\\lib\\site-packages (from -r requirements.txt (line 4)) (3.0.10)\n",
      "Requirement already satisfied: python-dotenv in c:\\python310\\lib\\site-packages (from -r requirements.txt (line 5)) (0.19.2)\n",
      "Requirement already satisfied: regex in c:\\python310\\lib\\site-packages (from -r requirements.txt (line 6)) (2022.9.13)\n",
      "Requirement already satisfied: sniffio in c:\\python310\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python310\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python310\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python310\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (0.25.2)\n",
      "Requirement already satisfied: tqdm>4 in c:\\python310\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\python310\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (1.10.13)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\python310\\lib\\site-packages (from openai->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\python310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\python310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2022.4)\n",
      "Requirement already satisfied: et-xmlfile in c:\\python310\\lib\\site-packages (from openpyxl->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python310\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (2022.9.24)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (0.13.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\d92343\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai->-r requirements.txt (line 1)) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import yaml\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert your API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = <your_api_key>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main Script for Document Analysis and Meta Data Generation\n",
    "\n",
    "This script performs the following steps:\n",
    "1. Load the scanned document from the specified directory.\n",
    "2. Analyze the document using the OpenAI GPT model.\n",
    "3. Generate meta data for the document including type, date, author, recipient,\n",
    "   title, keywords, summary, entities, language, and page count.\n",
    "4. Save the meta data as a YAML file in the same directory as the document.\n",
    "5. Generate an informative file name based on the meta data.\n",
    "6. Rename the document file with the newly generated informative name.\n",
    "7. Finally, create a csv and excel file with all the meta data.\n",
    "\n",
    "Project Structure:\n",
    "- database/\n",
    "- documents/\n",
    "  - <scanned_documents>.pdf\n",
    "- prompts/\n",
    "  - analyze_document.txt\n",
    "  - generate_title.txt\n",
    "- .env\n",
    "- .gitignore\n",
    "- main.py\n",
    "- README.md\n",
    "\n",
    "Instructions:\n",
    "1. Ensure you have the OpenAI API key in the .env file as\n",
    "   OPENAI_API_KEY=<your_api_key>.\n",
    "2. Place your scanned documents in the 'documents' directory.\n",
    "3. Modify the prompt in 'prompts/analyze_document.txt'.\n",
    "4. Run the script: python main.py\n",
    "\"\"\"\n",
    "\n",
    "# Function to load prompts from files\n",
    "def load_prompt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "# Function to analyze the document and extract meta data\n",
    "def analyze_document(file_path, prompt):\n",
    "    # create image data url\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        image_url = f\"data:image/jpeg;base64,{base64.b64encode(file.read()).decode()}\"  # noqa: E501\n",
    "        image_url = # TODO! Hint: How can you send an image via an API?\n",
    "\n",
    "    # error most likely request limit, first wait 5 seconds\n",
    "    time.sleep(5)\n",
    "\n",
    "    # TODO! Hint: Use ChatGPT playground or Docs\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": image_url\n",
    "                            },\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt,\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_tokens=256,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        # try again\n",
    "        time.sleep(15)\n",
    "        return analyze_document(file_path, prompt)\n",
    "\n",
    "\n",
    "# Function to generate a filename based on meta data\n",
    "def generate_filename(meta_data):\n",
    "    # <dokumententyp>_<autor>_<datum>_<titel>.jpg\n",
    "    # if any of the fields are missing, use \"unbekannt\"\n",
    "    string = \"\"\n",
    "    string += str(meta_data.get(\"document_type\", \"unbekannt\")) + \"_\"\n",
    "    string += str(meta_data.get(\"author\", \"unbekannt\")) + \"_\"\n",
    "    string += str(meta_data.get(\"date\", \"unbekannt\")) + \"_\"\n",
    "    string += str(meta_data.get(\"title\", \"unbekannt\"))\n",
    "    # Make sure that the filename is a valid filename\n",
    "    string = re.sub(r\"[^\\w\\s-]\", \"\", string)\n",
    "    return string + \".jpg\"\n",
    "\n",
    "\n",
    "# Function to create meta data file\n",
    "def create_meta_file(info, original_filename, renamed_filename):\n",
    "    meta_data = {\n",
    "        \"original_filename\": original_filename,\n",
    "        \"renamed_filename\": renamed_filename,\n",
    "        \"document_type\": info[\"document_type\"],\n",
    "        \"date\": info[\"date\"],\n",
    "        \"author\": info[\"author\"],\n",
    "        \"recipient\": info[\"recipient\"],\n",
    "        \"title\": info[\"title\"],\n",
    "        \"keywords\": info[\"keywords\"],\n",
    "        \"summary\": info[\"summary\"],\n",
    "        \"entities\": info[\"entities\"],\n",
    "        \"page\": info[\"page_count\"],\n",
    "        \"creation_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    }\n",
    "    return meta_data\n",
    "\n",
    "\n",
    "# Function to save meta data to a YAML file\n",
    "def save_meta_file(meta_data, meta_file_path):\n",
    "    # Save json file as yaml\n",
    "    with open(meta_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        yaml.dump(meta_data, file, default_flow_style=False,\n",
    "                  allow_unicode=True)\n",
    "\n",
    "\n",
    "# Function to save meta data to CSV and Excel files\n",
    "def save_meta_data_to_csv_excel(meta_data_list, csv_path, excel_path):\n",
    "    keys = meta_data_list[0].keys()\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(meta_data_list)\n",
    "\n",
    "    df = pd.DataFrame(meta_data_list)\n",
    "    df.to_excel(excel_path, index=False)\n",
    "\n",
    "\n",
    "def preprocess_yaml(yaml_string):\n",
    "    return re.sub(r'(?<=: )([^\"\\n]*: [^\"\\n]*)', r'\"\\1\"', yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure that database/meta_files and database/renamed_files exist\n",
    "if not os.path.exists('database/meta_files'):\n",
    "    os.makedirs('database/meta_files')\n",
    "else:\n",
    "    for file in os.listdir('database/meta_files'):\n",
    "        os.remove(os.path.join('database/meta_files', file))\n",
    "if not os.path.exists('database/renamed_files'):\n",
    "    os.makedirs('database/renamed_files')\n",
    "else:\n",
    "    for file in os.listdir('database/renamed_files'):\n",
    "        os.remove(os.path.join('database/renamed_files', file))\n",
    "\n",
    "document_dir = \"documents\"\n",
    "analyze_prompt_path = \"prompts/analyze_document.txt\"\n",
    "analyze_prompt = load_prompt(analyze_prompt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_list = []\n",
    "for filename in os.listdir(document_dir):\n",
    "    if (\n",
    "        filename.endswith(\".pdf\")\n",
    "        or filename.endswith(\".jpg\")\n",
    "        or filename.endswith(\".jpeg\")\n",
    "    ):\n",
    "        file_path = os.path.join(document_dir, filename)\n",
    "        print(f\"Processing {filename}...\")\n",
    "        # Analyze the document to get meta data\n",
    "        meta_info = analyze_document(file_path, analyze_prompt)\n",
    "        # Get content between \"```yaml\" and \"```\" to extract meta data\n",
    "        meta_info = meta_info.split(\"```yaml\")[1].split(\"```\")[0]\n",
    "        meta_info = preprocess_yaml(meta_info)\n",
    "        meta_json = yaml.safe_load(meta_info)\n",
    "        # Generate new file name\n",
    "        new_filename = generate_filename(meta_json)\n",
    "        # Rename the document file\n",
    "        renamed_file_location = os.path.join('database', 'renamed_files',\n",
    "                                             new_filename)\n",
    "        shutil.copy2(file_path, renamed_file_location)\n",
    "        # Create meta data file\n",
    "        meta_file_path = os.path.join('database', 'meta_files',\n",
    "                                      new_filename.split('.')[0] + \".yaml\")\n",
    "        save_meta_file(meta_json, meta_file_path)\n",
    "        # Add new file name to meta data\n",
    "        meta_json[\"renamed_filename\"] = new_filename\n",
    "        meta_data_list.append(meta_json)\n",
    "        print(f\"Processed {filename}, saved meta file as {meta_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all meta data to CSV and Excel files\n",
    "csv_path = \"database/meta_data.csv\"\n",
    "excel_path = \"database/meta_data.xlsx\"\n",
    "save_meta_data_to_csv_excel(meta_data_list, csv_path, excel_path)\n",
    "print(f\"Meta data saved to {csv_path} and {excel_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
